# -*- coding: utf-8 -*-
"""DetectGpt.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gb0hz9z8Enwy6nZ4FNWQ75KBYx-E3fs8

# Install Required Libraries
"""

pip install transformers datasets scikit-learn torch

"""**Load Dataset**"""

import pandas as pd
from sklearn.model_selection import train_test_split

data = pd.read_csv("/content/AI_Human.csv") #20,000+

train_texts, val_texts, train_labels, val_labels = train_test_split(
    data['text'], data['generated'], test_size=0.2, random_state=42
)

data.head()

data.describe()

import seaborn as sns

sns.countplot(data=data,x='generated')

"""# **Preprocess The Data**"""

# Drop rows with missing values
data = data.dropna(subset=['text', 'generated'])

# Remove duplicates
data = data.drop_duplicates(subset=['text'])

import re

def clean_text(text):
    text = text.lower()  # Convert to lowercase
    text = re.sub(r"http\S+|www\S+|https\S+", '', text, flags=re.MULTILINE)  # Remove URLs
    text = re.sub(r'\@\w+|\#', '', text)  # Remove mentions and hashtags
    text = re.sub(r'[^a-zA-Z\s]', '', text)  # Remove special characters and numbers
    text = re.sub(r'\s+', ' ', text).strip()  # Remove extra spaces
    return text


data['text'] = data['text'].apply(clean_text)

data = data[data['text'].str.len() > 10]

data['text'][0]

data['generated'] = data['generated'].astype(int)

"""Split Data into taining and validation sets"""

train_texts, val_texts, train_labels, val_labels = train_test_split(
    data['text'], data['generated'], test_size=0.2, random_state=42
)

"""Tokenize The data"""

from transformers import AutoTokenizer


tokenizer = AutoTokenizer.from_pretrained("distilbert-base-uncased")


train_encodings = tokenizer(list(train_texts), truncation=True, padding=True, max_length=128)
val_encodings = tokenizer(list(val_texts), truncation=True, padding=True, max_length=128)

"""# Prepare Datasets"""

import torch

class Dataset(torch.utils.data.Dataset):
    def __init__(self, encodings, labels):
        self.encodings = encodings
        self.labels = labels

    def __len__(self):
        return len(self.labels)

    def __getitem__(self, idx):
        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}
        item['labels'] = torch.tensor(self.labels[idx])
        return item


train_dataset = Dataset(train_encodings, list(train_labels))
val_dataset = Dataset(val_encodings, list(val_labels))

"""# Load Pre-trained Model"""

from transformers import AutoModelForSequenceClassification


model = AutoModelForSequenceClassification.from_pretrained("distilbert-base-uncased", num_labels=2)

"""Define Training **arguments**"""

from transformers import Trainer, TrainingArguments
from transformers import EarlyStoppingCallback
training_args = TrainingArguments(
    output_dir="./results",
    evaluation_strategy="epoch",
    logging_strategy="steps",
    logging_steps=100,
      lr_scheduler_type="linear",
    save_strategy="epoch",
    learning_rate=2e-5,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    num_train_epochs=3,
    weight_decay=0.01,
    logging_dir="./logs",
    report_to=[]
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
    callbacks=[EarlyStoppingCallback(early_stopping_patience=1)]
)

pip uninstall wandb

"""Training"""

trainer.train()

"""Evaluation"""

# Evaluate the model
results = trainer.evaluate()
print(results)

"""# Prediction"""

test_texts = ["""Cars. Cars have been around since they became famous in the 1900s, when Henry Ford created and built the first ModelT. Cars have played a major role in our every day lives since then. But now, people are starting to question if limiting car usage would be a good thing. To me, limiting the use of cars might be a good thing to do.

In like matter of this, article, "In German Suburb, Life Goes On Without Cars," by Elizabeth Rosenthal states, how automobiles are the linchpin of suburbs, where middle class families from either Shanghai or Chicago tend to make their homes. Experts say how this is a huge impediment to current efforts to reduce greenhouse gas emissions from tailpipe. Passenger cars are responsible for 12 percent of greenhouse gas emissions in Europe...and up to 50 percent in some carintensive areas in the United States. Cars are the main reason for the greenhouse gas emissions because of a lot of people driving them around all the time getting where they need to go. Article, "Paris bans driving due to smog," by Robert Duffer says, how Paris, after days of nearrecord pollution, enforced a partial driving ban to clear the air of the global city. It also says, how on Monday, motorist with evennumbered license plates were ordered to leave their cars at home or be fined a 22euro fine 31. The same order would be applied to oddnumbered plates the following day. Cars are the reason for polluting entire cities like Paris. This shows how bad cars can be because, of all the pollution that they can cause to an entire city.

Likewise, in the article, "Carfree day is spinning into a big hit in Bogota," by Andrew Selsky says, how programs that's set to spread to other countries, millions of Columbians hiked, biked, skated, or took the bus to work during a carfree day, leaving streets of this capital city eerily devoid of traffic jams. It was the third straight year cars have been banned with only buses and taxis permitted for the Day Without Cars in the capital city of 7 million. People like the idea of having carfree days because, it allows them to lesson the pollution that cars put out of their exhaust from people driving all the time. The article also tells how parks and sports centers have bustled throughout the city uneven, pitted sidewalks have been replaced by broad, smooth sidewalks rushhour restrictions have dramatically cut traffic and new restaurants and upscale shopping districts have cropped up. Having no cars has been good for the country of Columbia because, it has aloud them to repair things that have needed repairs for a long time, traffic jams have gone down, and restaurants and shopping districts have popped up, all due to the fact of having less cars around.

In conclusion, the use of less cars and having carfree days, have had a big impact on the environment of cities because, it is cutting down the air pollution that the cars have majorly polluted, it has aloud countries like Columbia to repair sidewalks, and cut down traffic jams. Limiting the use of cars would be a good thing for America. So we should limit the use of cars by maybe riding a bike, or maybe walking somewhere that isn't that far from you and doesn't need the use of a car to get you there. To me, limiting the use of cars might be a good thing to do.""", "Dear Senator "]
test_labels = [0, 1]


test_encodings = tokenizer(test_texts, truncation=True, padding=True, max_length=128)
test_dataset = Dataset(test_encodings, test_labels)


prediction_output = trainer.predict(test_dataset)

logits = prediction_output.predictions
true_labels = prediction_output.label_ids

predicted_classes = np.argmax(logits, axis=1)

accuracy = np.mean(predicted_classes == true_labels)


print("Logits:\n", logits)
print("\nPredicted Classes:", predicted_classes)
print("\nTrue Labels:", true_labels)

"""Training and Validation Loss Visualization"""

import matplotlib.pyplot as plt

# Extract the training history
train_loss = [log["loss"] for log in trainer.state.log_history if "loss" in log]
eval_loss = [log["eval_loss"] for log in trainer.state.log_history if "eval_loss" in log]

# Adjust epochs for eval_loss to match its length
eval_epochs = range(1, len(train_loss) + 1, int(len(train_loss) / len(eval_loss)))

# Create the plot
plt.figure(figsize=(12, 7))
plt.plot(
    range(1, len(train_loss) + 1),
    train_loss,
    label="Training Loss",
    marker="o",
    color="#1f77b4",
    linewidth=2,
)
plt.plot(
    eval_epochs,
    eval_loss,
    label="Validation Loss",
    marker="s",
    color="#ff7f0e",
    linewidth=2,
)


plt.grid(visible=True, linestyle="--", alpha=0.6, color="gray")
plt.gca().set_facecolor("#f9f9f9")


plt.xlabel("Epoch", fontsize=12, fontweight="bold")
plt.ylabel("Loss", fontsize=12, fontweight="bold")
plt.title("Training and Validation Loss Over Epochs", fontsize=14, fontweight="bold")


plt.legend(fontsize=11, loc="upper right", frameon=True, shadow=True, fancybox=True)

plt.tight_layout()
plt.show()

"""# Confusion Matrix"""

import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix
import seaborn as sns


logits = prediction_output.predictions
true_labels = prediction_output.label_ids


predicted_classes = np.argmax(logits, axis=1)


cm = confusion_matrix(true_labels, predicted_classes)

plt.figure(figsize=(6, 5))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=["Human", "AI-generated"], yticklabels=["Human", "AI-generated"])
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Confusion Matrix')
plt.show()

print("Confusion Matrix:")
print(cm)